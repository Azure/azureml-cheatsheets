<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.0">
<link rel="alternate" type="application/rss+xml" href="/azureml-cheatsheets/blog/rss.xml" title="Azure Machine Learning Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/azureml-cheatsheets/blog/atom.xml" title="Azure Machine Learning Blog Atom Feed">
<link rel="search" type="application/opensearchdescription+xml" title="Azure Machine Learning" href="/azureml-cheatsheets/opensearch.xml">
<script async>!function(e,t,n){var a=e.location,i="script",r="instrumentationKey",o="ingestionendpoint",s="disableExceptionTracking",c="ai.device.",u="toLowerCase",p="crossOrigin",l="POST",d="appInsightsSDK",g=n.name||"appInsights";(n.name||e[d])&&(e[d]=g);var f=e[g]||function(d){var g=!1,f=!1,m={initialize:!0,queue:[],sv:"4",version:2,config:d};function v(e,t){var n={},i="Browser";return n[c+"id"]=i[u](),n[c+"type"]=i,n["ai.operation.name"]=a&&a.pathname||"_unknown_",n["ai.internal.sdkVersion"]="javascript:snippet_"+(m.sv||m.version),{time:function(){var e=new Date;function t(e){var t=""+e;return 1===t.length&&(t="0"+t),t}return e.getUTCFullYear()+"-"+t(1+e.getUTCMonth())+"-"+t(e.getUTCDate())+"T"+t(e.getUTCHours())+":"+t(e.getUTCMinutes())+":"+t(e.getUTCSeconds())+"."+((e.getUTCMilliseconds()/1e3).toFixed(3)+"").slice(2,5)+"Z"}(),iKey:e,name:"Microsoft.ApplicationInsights."+e.replace(/-/g,"")+"."+t,sampleRate:100,tags:n,data:{baseData:{ver:2}}}}var h=d.url||n.src;if(h){function y(t){var a,i,s,c,p,y,T,S,b;g=!0,m.queue=[],f||(f=!0,a=h,p=function(){var e={},t=d.connectionString;if(t)for(var n=t.split(";"),a=0;a<n.length;a++){var i=n[a].split("=");2===i.length&&(e[i[0][u]()]=i[1])}if(!e[o]){var r=e.endpointsuffix,s=r?e.location:null;e[o]="https://"+(s?s+".":"")+"dc."+(r||"services.visualstudio.com")}return e}(),y=p[r]||d[r]||"",S=(T=p[o])?T+"/v2/track":config.endpointUrl,(b=[]).push((i="SDK LOAD Failure: Failed to load Application Insights SDK script (See stack for details)",a,S,(c=(s=v(y,"Exception")).data).baseType="ExceptionData",c.baseData.exceptions=[{typeName:"SDKLoadFailed",message:i.replace(/./g,"-"),hasFullStack:!1,stack:i,parsedStack:[]}],s)),b.push(function(e,t,n,a){var i=v(y,"Message"),r=i.data;r.baseType="MessageData";var o=r.baseData;return o.message='AI (Internal): 99 message:"'+("SDK LOAD Failure: Failed to load Application Insights SDK script (See stack for details) ("+n+")").replace(/"/g,"")+'"',o.properties={endpoint:a},i}(0,0,a,S)),function(t,a){if(JSON){var i=e.fetch;if(i&&!n.useXhr)i(a,{method:l,body:JSON.stringify(t),mode:"cors"});else if(XMLHttpRequest){var r=new XMLHttpRequest;r.open(l,a),r.setRequestHeader("Content-type","application/json"),r.send(JSON.stringify(t))}}}(b,S))}function T(e,t){f||setTimeout((function(){!t&&m.core||y()}),500)}var S=function(){var e=t.createElement(i);e.src=h;var a=n[p];return!a&&""!==a||"undefined"==e[p]||(e[p]=a),e.onload=T,e.onerror=y,e.onreadystatechange=function(t,n){"loaded"!==e.readyState&&"complete"!==e.readyState||T(0,n)},e}();n.ld<0?t.getElementsByTagName("head")[0].appendChild(S):setTimeout((function(){t.getElementsByTagName(i)[0].parentNode.appendChild(S)}),n.ld||0)}try{m.cookie=t.cookie}catch(e){}function b(e){for(;e.length;)!function(e){m[e]=function(){var t=arguments;g||m.queue.push((function(){m[e].apply(m,t)}))}}(e.pop())}var D="track",k="TrackPage",C="TrackEvent";b([D+"Event",D+"PageView",D+"Exception",D+"Trace",D+"DependencyData",D+"Metric",D+"PageViewPerformance","start"+k,"stop"+k,"start"+C,"stop"+C,"addTelemetryInitializer","setAuthenticatedUserContext","clearAuthenticatedUserContext","flush"]),m.SeverityLevel={Verbose:0,Information:1,Warning:2,Error:3,Critical:4};var x=(d.extensionConfig||{}).ApplicationInsightsAnalytics||{};if(!0!==d[s]&&!0!==x[s]){method="onerror",b(["_"+method]);var A=e[method];e[method]=function(e,t,n,a,i){var r=A&&A(e,t,n,a,i);return!0!==r&&m["_"+method]({message:e,url:t,lineNumber:n,columnNumber:a,error:i}),r},d.autoExceptionInstrumented=!0}return m}(n.cfg);(e[g]=f).queue&&0===f.queue.length&&f.trackPageView({})}(window,document,{src:"https://az416426.vo.msecnd.net/scripts/b/ai.2.min.js",cfg:{instrumentationKey:"cf50e24d-cb96-4b51-b40b-c5ba218e4fd0",enableAutoRouteTracking:!0}})</script><title data-react-helmet="true">Distributed GPU Training | Azure Machine Learning</title><meta data-react-helmet="true" property="og:url" content="https://github.com/Azure//azureml-cheatsheets/docs/cheatsheets/python/v1/distributed-training"><meta data-react-helmet="true" name="docsearch:language" content="en"><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Distributed GPU Training | Azure Machine Learning"><meta data-react-helmet="true" name="description" content="Guide to distributed training in Azure ML."><meta data-react-helmet="true" property="og:description" content="Guide to distributed training in Azure ML."><meta data-react-helmet="true" name="keywords" content="distributed training,mpi,process group,pytorch,horovod,tensorflow"><link data-react-helmet="true" rel="shortcut icon" href="/azureml-cheatsheets/img/logo.svg"><link data-react-helmet="true" rel="canonical" href="https://github.com/Azure//azureml-cheatsheets/docs/cheatsheets/python/v1/distributed-training"><link data-react-helmet="true" rel="alternate" href="https://github.com/Azure//azureml-cheatsheets/docs/cheatsheets/python/v1/distributed-training" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://github.com/Azure//azureml-cheatsheets/docs/cheatsheets/python/v1/distributed-training" hreflang="x-default"><link data-react-helmet="true" rel="preconnect" href="https://BH4D9OD16A-dsn.algolia.net" crossorigin="anonymous"><link rel="stylesheet" href="/azureml-cheatsheets/assets/css/styles.af70c88a.css">
<link rel="preload" href="/azureml-cheatsheets/assets/js/runtime~main.421f25ac.js" as="script">
<link rel="preload" href="/azureml-cheatsheets/assets/js/main.ea6aa2e5.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#main" class="skipToContent_1oUP">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle" type="button" tabindex="0"><svg aria-label="Menu" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/azureml-cheatsheets/"><img src="/azureml-cheatsheets/img/logo.svg" alt="Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/azureml-cheatsheets/img/logo.svg" alt="Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><strong class="navbar__title">Azure Machine Learning</strong></a><a class="navbar__item navbar__link" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/cheatsheet">Python SDK</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle displayOnlyInLargeViewport_GrZ2 react-toggle--disabled" role="button" tabindex="-1"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_71bT">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_71bT">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" class="react-toggle-screenreader-only" aria-label="Switch between dark and light mode"></div><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span></button></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/azureml-cheatsheets/"><img src="/azureml-cheatsheets/img/logo.svg" alt="Logo" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/azureml-cheatsheets/img/logo.svg" alt="Logo" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><strong class="navbar__title">Azure Machine Learning</strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/cheatsheet">Python SDK</a></li></ul></div></div></div></nav><div class="main-wrapper docs-wrapper doc-page"><div class="docPage_31aa"><div class="docSidebarContainer_3Kbt" role="complementary"><div class="sidebar_15mo"><div class="menu menu--responsive thin-scrollbar menu_Bmed"><button aria-label="Open menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_fgN0" width="24" height="24" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Python</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/cheatsheet">Cheat Sheet</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist" href="#!" tabindex="0">Getting Started</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/installation">Installation</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist" href="#!" tabindex="0">Azure ML Resources</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/workspace">Workspace</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/compute-targets">Compute Target</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/environment">Environment</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/data">Data</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!" tabindex="0">Guides</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/script-run-config">Running Code in the Cloud</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/logging">Metrics</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/distributed-training">Distributed GPU Training</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/docker-build">Azure ML Containers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/debugging">Debugging</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/ci-dev">Developing on Azure ML</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/troubleshooting">Troubleshooting</a></li></ul></li></ul></li></ul></div></div></div><main class="docMainContainer_3ufF"><div class="container padding-vert--lg docItemWrapper_3FMP"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><header><h1 class="docTitle_3a4h">Distributed GPU Training</h1></header><div class="markdown"><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="basic-concepts"></a>Basic Concepts<a class="hash-link" href="#basic-concepts" title="Direct link to heading">#</a></h2><p>We assume readers already understand the basic concept of distributed GPU training such as <em>data parallelism, distributed data parallelism, and model parallelism</em>. This guide aims at helping readers running existing distributed training code on Azure ML. </p><div class="admonition admonition-info alert alert--info"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>info</h5></div><div class="admonition-content"><p>If you don&#x27;t know which type of parallelism to use, for &gt;90% of the time you should use <strong>Distributed Data Parallelism</strong>.</p></div></div><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="mpi"></a>MPI<a class="hash-link" href="#mpi" title="Direct link to heading">#</a></h2><p>Azure ML offers an MPI job to launch a given number of processes in each node. Users can adopt this approach to run distributed training using either per-process-launcher or per-node-launcher, depending on whether <code>process_count_per_node</code> is set to 1 (the default) for per-node-launcher, or equal to the number of devices/GPUs for per-process-launcher. Azure ML handles constructing the full MPI launch command (<code>mpirun</code>) behind the scenes.</p><div class="admonition admonition-note alert alert--secondary"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="16" viewBox="0 0 14 16"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg></span>note</h5></div><div class="admonition-content"><p>Azure ML currently does not allow users to provide the full head-node-launcher command like <code>mpirun</code> or the DeepSpeed launcher. This functionality may be added in a future release.</p></div></div><div class="admonition admonition-caution alert alert--warning"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</h5></div><div class="admonition-content"><p>To use the Azure ML MPI job, the base Docker image used by the job needs to have an MPI library installed. <a href="https://www.open-mpi.org/" target="_blank" rel="noopener noreferrer">Open MPI</a> is included in all the <a href="https://github.com/Azure/AzureML-Containers" target="_blank" rel="noopener noreferrer">AzureML GPU base images</a>. If you are using a custom Docker image, you are responsible for making sure the image includes an MPI library. Open MPI is recommended, but you can also use a different MPI implementation such as Intel MPI. Azure ML also provides <a href="https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments" target="_blank" rel="noopener noreferrer">curated environments</a> for popular frameworks. </p></div></div><p>To run distributed training using MPI, follow these steps:</p><ol><li>Use an Azure ML environment with the preferred deep learning framework and MPI. AzureML provides <a href="https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments" target="_blank" rel="noopener noreferrer">curated environment</a> for popular frameworks.</li><li>Define <code>MpiConfiguration</code> with the desired <code>process_count_per_node</code> and <code>node_count</code>. <code>process_count_per_node</code> should be equal to the number of GPUs per node for per-process-launch, or set to 1 (the default) for per-node-launch if the user script will be responsible for launching the processes per node.</li><li>Pass the <code>MpiConfiguration</code> object to the <code>distributed_job_config</code> parameter of <code>ScriptRunConfig</code>.</li></ol><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly python"><div tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> azureml</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">core </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> Workspace</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> ScriptRunConfig</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> Environment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> Experiment</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> azureml</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">core</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">runconfig </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> MpiConfiguration</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">curated_env_name </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;AzureML-PyTorch-1.6-GPU&#x27;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">pytorch_env </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> Environment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">get</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">workspace</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">ws</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">curated_env_name</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">distr_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> MpiConfiguration</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">process_count_per_node</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">4</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> node_count</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">2</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">run_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> ScriptRunConfig</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  source_directory</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;./src&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  script</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;train.py&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  compute_target</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">compute_target</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  environment</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">pytorch_env</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  distributed_job_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">distr_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># submit the run configuration to start the job</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">run </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> Experiment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">ws</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;experiment_name&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">submit</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">run_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="horovod"></a>Horovod<a class="hash-link" href="#horovod" title="Direct link to heading">#</a></h3><p>If you are using <a href="https://horovod.readthedocs.io/en/stable/index.html" target="_blank" rel="noopener noreferrer">Horovod</a> for distributed training with the deep learning framework of your choice, you can run distributed training on Azure ML using the MPI job configuration.</p><p>Simply ensure that you have taken care of the following:</p><ul><li>The training code is instrumented correctly with Horovod.</li><li>Your Azure ML environment contains Horovod and MPI. The PyTorch and TensorFlow curated GPU environments come pre-configured with Horovod and its dependencies.</li><li>Create an <code>MpiConfiguration</code> with your desired distribution.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="example"></a>Example<a class="hash-link" href="#example" title="Direct link to heading">#</a></h4><ul><li><a href="https://github.com/Azure/azureml-examples/tree/main/workflows/train/tensorflow/mnist-distributed-horovod" target="_blank" rel="noopener noreferrer">azureml-examples: TensorFlow distributed training using Horovod</a></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="deepspeed"></a>DeepSpeed<a class="hash-link" href="#deepspeed" title="Direct link to heading">#</a></h3><p>To run distributed training with the <a href="https://www.deepspeed.ai/" target="_blank" rel="noopener noreferrer">DeepSpeed</a> library on Azure ML, do not use DeepSpeed&#x27;s custom launcher. Instead, configure an MPI job to launch the training job <a href="https://www.deepspeed.ai/getting-started/#mpi-and-azureml-compatibility" target="_blank" rel="noopener noreferrer">with MPI</a>.</p><p>Ensure that you have taken care of the following:</p><ul><li>Your Azure ML environment contains DeepSpeed and its dependencies, Open MPI, and mpi4py.</li><li>Create an <code>MpiConfiguration</code> with your desired distribution.</li></ul><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="example-1"></a>Example<a class="hash-link" href="#example-1" title="Direct link to heading">#</a></h4><ul><li><a href="https://github.com/Azure/azureml-examples/tree/main/workflows/train/deepspeed/cifar" target="_blank" rel="noopener noreferrer">azureml-examples: Distributed training with DeepSpeed on CIFAR-10</a></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="environment-variables-from-open-mpi"></a>Environment variables from Open MPI<a class="hash-link" href="#environment-variables-from-open-mpi" title="Direct link to heading">#</a></h3><p>When running MPI jobs with Open MPI images, the following environment variables for each process launched:</p><ol><li>OMPI_COMM_WORLD_RANK - the rank of the process</li><li>OMPI_COMM_WORLD_SIZE - the world size</li><li>AZ_BATCH_MASTER_NODE - master address with port, MASTER_ADDR:MASTER_PORT</li><li>OMPI_COMM_WORLD_LOCAL_RANK - the local rank of the process on the node</li><li>OMPI_COMM_WORLD_LOCAL_SIZE - number of processes on the node</li></ol><div class="admonition admonition-caution alert alert--warning"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</h5></div><div class="admonition-content"><p>Despite the name, environment variable OMPI_COMM_WORLD_NODE_RANK does not corresponds to the NODE_RANK. To use per-node-launcher, simply set <code>process_count_per_node=1</code> and use OMPI_COMM_WORLD_RANK as the NODE_RANK. </p></div></div><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="pytorch"></a>PyTorch<a class="hash-link" href="#pytorch" title="Direct link to heading">#</a></h2><p>Azure ML also supports running distributed jobs using PyTorch&#x27;s native distributed training capabilities (<code>torch.distributed</code>).</p><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>torch.nn.parallel.DistributedDataParallel vs torch.nn.DataParallel and torch.multiprocessing</h5></div><div class="admonition-content"><p>For data parallelism, the <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel" target="_blank" rel="noopener noreferrer">official PyTorch guidance</a> is to use DistributedDataParallel (DDP) over DataParallel for both single-node and multi-node distributed training. PyTorch also <a href="https://pytorch.org/docs/stable/notes/cuda.html#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel" target="_blank" rel="noopener noreferrer">recommends using DistributedDataParallel over the multiprocessing package</a>. Azure ML documentation and examples will therefore focus on DistributedDataParallel training.</p></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="process-group-initialization"></a>Process group initialization<a class="hash-link" href="#process-group-initialization" title="Direct link to heading">#</a></h3><p>The backbone of any distributed training is based on a group of processes that know each other and can communicate with each other using a backend. For PyTorch, the process group is created by calling <a href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group" target="_blank" rel="noopener noreferrer">torch.distributed.init_process_group</a> in <strong>all distributed processes</strong> to collectively form a process group.</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly"><div tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">torch.distributed.init_process_group(backend=&#x27;nccl&#x27;, init_method=&#x27;env://&#x27;, ...)</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><p>The most common communication backends used are <strong>mpi</strong>, <strong>nccl</strong> and <strong>gloo</strong>. For GPU-based training <strong>nccl</strong> is strongly recommended for best performance and should be used whenever possible. </p><p><code>init_method</code> specifies how each process can discover each other and initialize as well as verify the process group using the communication backend. By default if <code>init_method</code> is not specified PyTorch will use the environment variable initialization method (<code>env://</code>). This is also the recommended the initialization method to use in your training code to run distributed PyTorch on Azure ML. For environment variable initialization, PyTorch will look for the following environment variables:</p><ul><li><strong>MASTER_ADDR</strong> - IP address of the machine that will host the process with rank 0.</li><li><strong>MASTER_PORT</strong> - A free port on the machine that will host the process with rank 0.</li><li><strong>WORLD_SIZE</strong> - The total number of processes. This should be equal to the total number of devices (GPU) used for distributed training.</li><li><strong>RANK</strong> - The (global) rank of the current process. The possible values are 0 to (world size - 1).</li></ul><p>For more information on process group initialization, see the <a href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group" target="_blank" rel="noopener noreferrer">PyTorch documentation</a>.</p><p>Beyond these, many applications will also need the following environment variables:</p><ul><li><strong>LOCAL_RANK</strong> - The local (relative) rank of the process within the node. The possible values are 0 to (# of processes on the node - 1). This information is useful because many operations such as data preparation only should be performed once per node --- usually on local_rank = 0.</li><li><strong>NODE_RANK</strong> - The rank of the node for multi-node training. The possible values are 0 to (total # of nodes - 1).</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="launch-options"></a>Launch options<a class="hash-link" href="#launch-options" title="Direct link to heading">#</a></h3><p>The Azure ML PyTorch job supports two types of options for launching distributed training:</p><ol><li><strong>Per-process-launcher</strong>: The system will launch all distributed processes for the user, with all the relevant information (e.g. environment variables) to set up the process group.</li><li><strong>Per-node-launcher</strong>: The user provides Azure ML with the utility launcher that will get run on each node. The utility launcher will handle launching each of the processes on a given node. Locally within each node, RANK and LOCAL_RANK is set up by the launcher. The <strong>torch.distributed.launch</strong> utility and PyTorch Lightning both belong in this category.</li></ol><p>There are no fundamental differences between these launch options; it is largely up to the user&#x27;s preference or the conventions of the frameworks/libraries built on top of vanilla PyTorch (such as Lightning or Hugging Face).</p><p>The following sections go into more detail on how to configure Azure ML PyTorch jobs for each of the launch options.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="distributeddataparallel-per-process-launch"></a>DistributedDataParallel (per-process-launch)<a class="hash-link" href="#distributeddataparallel-per-process-launch" title="Direct link to heading">#</a></h3><p>Azure ML supports launching each process for the user without the user needing to use a launcher utility like <code>torch.distributed.launch</code>.</p><p>To run a distributed PyTorch job, you will just need to do the following:</p><ol><li>Specify the training script and arguments</li><li>Create a <code>PyTorchConfiguration</code> and specify the <code>process_count</code> as well as the <code>node_count</code>. The <code>process_count</code> corresponds to the total number of processes you want to run for your job. This should typically equal <code># GPUs per node x # nodes</code>. If <code>process_count</code> is not specified, Azure ML will by default launch one process per node.</li></ol><p>Azure ML will set the MASTER_ADDR, MASTER_PORT, WORLD_SIZE, and NODE_RANK environment variables on each node, in addition to setting the process-level RANK and LOCAL_RANK environment variables.</p><div class="admonition admonition-caution alert alert--warning"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>caution</h5></div><div class="admonition-content"><p>In order to use this option for multi-process-per-node training, you will need to use Azure ML Python SDK <code>&gt;= 1.22.0</code>, as process_count was introduced in 1.22.0.</p></div></div><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly python"><div tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> azureml</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">core </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> ScriptRunConfig</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> Environment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> Experiment</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> azureml</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">core</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">runconfig </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> PyTorchConfiguration</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">curated_env_name </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;AzureML-PyTorch-1.6-GPU&#x27;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">pytorch_env </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> Environment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">get</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">workspace</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">ws</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">curated_env_name</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">distr_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> PyTorchConfiguration</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">process_count</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">8</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> node_count</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">2</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">run_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> ScriptRunConfig</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  source_directory</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;./src&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  script</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;train.py&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  arguments</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;--epochs&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">50</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  compute_target</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">compute_target</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  environment</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">pytorch_env</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  distributed_job_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">distr_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">run </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> Experiment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">ws</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;experiment_name&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">submit</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">run_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</h5></div><div class="admonition-content"><p>If your training script passes information like local rank or rank as script arguments, you can reference the environment variable(s) in the arguments:
<code>arguments=[&#x27;--epochs&#x27;, 50, &#x27;--local_rank&#x27;, $LOCAL_RANK]</code>. </p></div></div><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="example-2"></a>Example<a class="hash-link" href="#example-2" title="Direct link to heading">#</a></h4><ul><li><a href="https://github.com/Azure/azureml-examples/tree/main/workflows/train/pytorch/cifar-distributed" target="_blank" rel="noopener noreferrer">azureml-examples: Distributed training with PyTorch on CIFAR-10</a></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="using-torchdistributedlaunch-per-node-launch"></a>Using <code>torch.distributed.launch</code> (per-node-launch)<a class="hash-link" href="#using-torchdistributedlaunch-per-node-launch" title="Direct link to heading">#</a></h3><p>PyTorch provides a launch utility in <a href="https://pytorch.org/docs/stable/distributed.html#launch-utility" target="_blank" rel="noopener noreferrer">torch.distributed.launch</a> that users can use to launch multiple processes per node. The <code>torch.distributed.launch</code> module will spawn multiple training processes on each of the nodes.</p><p>The following steps will demonstrate how to configure a PyTorch job with a per-node-launcher on Azure ML that will achieve the equivalent of running the following command:</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly"><div tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">python -m torch.distributed.launch --nproc_per_node &lt;num processes per node&gt; \</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  --nnodes &lt;num nodes&gt; --node_rank $NODE_RANK --master_addr $MASTER_ADDR \</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  --master_port $MASTER_PORT --use_env \</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  &lt;your training script&gt; &lt;your script arguments&gt;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><ol><li>Provide the <code>torch.distributed.launch</code> command to the <code>command</code> parameter of the <code>ScriptRunConfig</code> constructor. Azure ML will run this command on each node of your training cluster. <code>--nproc_per_node</code> should be less than or equal to the number of GPUs available on each node. MASTER_ADDR, MASTER_PORT, and NODE_RANK are all set by Azure ML, so you can just reference the environment variables in the command. Azure ML sets MASTER_PORT to <code>6105</code>, but you can pass a different value to the <code>--master_port</code> argument of torch.distributed.launch command if you wish. (The launch utility will reset the environment variables.)</li><li>Create a <code>PyTorchConfiguration</code> and specify the <code>node_count</code>.</li></ol><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly python"><div tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> azureml</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">core </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> ScriptRunConfig</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> Environment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> Experiment</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> azureml</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">core</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">runconfig </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> PyTorchConfiguration</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">curated_env_name </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;AzureML-PyTorch-1.6-GPU&#x27;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">pytorch_env </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> Environment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">get</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">workspace</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">ws</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">curated_env_name</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">distr_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> PyTorchConfiguration</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">node_count</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">2</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">launch_cmd </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;python -m torch.distributed.launch --nproc_per_node 4 --nnodes 2 --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT --use_env train.py --epochs 50&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">split</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">run_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> ScriptRunConfig</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  source_directory</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;./src&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  command</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">launch_cmd</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  compute_target</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">compute_target</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  environment</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">pytorch_env</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  distributed_job_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">distr_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">run </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> Experiment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">ws</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;experiment_name&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">submit</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">run_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><div class="admonition admonition-tip alert alert--success"><div class="admonition-heading"><h5><span class="admonition-icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="16" viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Single-node multi-GPU training</h5></div><div class="admonition-content"><p>If you are using the launch utility to run single-node multi-GPU PyTorch training, you do not need to specify the <code>distributed_job_config</code> parameter of ScriptRunConfig.</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly python"><div tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">launch_cmd </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;python -m torch.distributed.launch --nproc_per_node 4 --use_env train.py --epochs 50&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">split</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">run_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> ScriptRunConfig</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  source_directory</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;./src&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  command</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">launch_cmd</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  compute_target</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">compute_target</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  environment</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">pytorch_env</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div></div></div><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="example-3"></a>Example<a class="hash-link" href="#example-3" title="Direct link to heading">#</a></h4><ul><li><a href="https://github.com/Azure/azureml-examples/tree/main/workflows/train/pytorch/cifar-distributed" target="_blank" rel="noopener noreferrer">azureml-examples: Distributed training with PyTorch on CIFAR-10</a></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="pytorch-lightning"></a>PyTorch Lightning<a class="hash-link" href="#pytorch-lightning" title="Direct link to heading">#</a></h3><p><a href="https://pytorch-lightning.readthedocs.io/en/stable/" target="_blank" rel="noopener noreferrer">PyTorch Lightning</a> is a lightweight open-source library that provides a high-level interface for PyTorch. Lightning abstracts away much of the lower-level distributed training configurations required for vanilla PyTorch from the user, and allows users to run their training scripts in single GPU, single-node multi-GPU, and multi-node multi-GPU settings. Behind the scene it launches multiple processes for user similar to <code>torch.distributed.launch</code>.</p><p>For single-node training (including single-node multi-GPU), you can run your code on Azure ML without needing to specify a <code>distributed_job_config</code>. For multi-node training, Lightning requires the following environment variables to be set on each node of your training cluster:</p><ul><li>MASTER_ADDR</li><li>MASTER_PORT</li><li>NODE_RANK</li></ul><p>To run multi-node Lightning training on Azure ML, you can largely follow the <a href="#using-distributedddataparallel-per-node-launch">per-node-launch guide</a>:</p><ul><li>Define the <code>PyTorchConfiguration</code> and specify the desired <code>node_count</code>. Do not specify <code>process_count</code> as Lightning internally handles launching the worker processes for each node.</li><li>For PyTorch jobs, Azure ML handles setting the MASTER_ADDR, MASTER_PORT, and NODE_RANK envirnment variables required by Lightning.</li><li>Lightning will handle computing the world size from the Trainer flags <code>--gpus</code> and <code>--num_nodes</code> and manage rank and local rank internally.</li></ul><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly python"><div tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> azureml</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">core </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> ScriptRunConfig</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> Experiment</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> azureml</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">core</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">runconfig </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> PyTorchConfiguration</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">nnodes </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">2</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">args </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;--max_epochs&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">50</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;--gpus&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">2</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;--accelerator&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;ddp&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;--num_nodes&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> nnodes</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">distr_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> PyTorchConfiguration</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">node_count</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">nnodes</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">run_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> ScriptRunConfig</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  source_directory</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;./src&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  script</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;train.py&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  arguments</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">args</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  compute_target</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">compute_target</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  environment</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">pytorch_env</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  distributed_job_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">distr_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">run </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> Experiment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">ws</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;experiment_name&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">submit</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">run_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="example-4"></a>Example<a class="hash-link" href="#example-4" title="Direct link to heading">#</a></h4><ul><li><a href="https://github.com/Azure/azureml-examples/blob/main/tutorials/using-pytorch-lightning/4.train-multi-node-ddp.ipynb" target="_blank" rel="noopener noreferrer">azureml-examples: Multi-node training with PyTorch Lightning</a></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="hugging-face-transformers"></a>Hugging Face Transformers<a class="hash-link" href="#hugging-face-transformers" title="Direct link to heading">#</a></h3><p>Hugging Face provides many <a href="https://github.com/huggingface/transformers/tree/master/examples" target="_blank" rel="noopener noreferrer">examples</a> for using its Transformers library with <code>torch.distributed.launch</code> to run distributed training. To run these examples and your own custom training scripts using the Transformers Trainer API, follow the <a href="#using-torchdistributedlaunch-per-node-launch">Using <code>torch.distributed.launch</code></a> section.</p><p>Sample job configuration code to fine-tune the BERT large model on the text classification MNLI task using the <code>run_glue.py</code> script on one node with 8 GPUs:</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly python"><div tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> azureml</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">core </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> ScriptRunConfig</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> azureml</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">core</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">runconfig </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> PyTorchConfiguration</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">distr_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> PyTorchConfiguration</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># node_count defaults to 1</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">launch_cmd </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;python -m torch.distributed.launch --nproc_per_node 8 text-classification/run_glue.py --model_name_or_path bert-large-uncased-whole-word-masking --task_name mnli --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 8 --learning_rate 2e-5 --num_train_epochs 3.0 --output_dir /tmp/mnli_output&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">split</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">run_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> ScriptRunConfig</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  source_directory</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;./src&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  command</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">launch_cmd</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  compute_target</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">compute_target</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  environment</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">pytorch_env</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  distributed_job_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">distr_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><p>You can also use the <a href="#distributeddataparallel-per-process-launch">per-process-launch</a> option to run distributed training without using <code>torch.distributed.launch</code>. One thing to keep in mind if using this method is that the transformers <a href="https://huggingface.co/transformers/main_classes/trainer.html?highlight=launch#trainingarguments" target="_blank" rel="noopener noreferrer">TrainingArguments</a> expects the local rank to be passed in as an argument (<code>--local_rank</code>). <code>torch.distributed.launch</code> takes care of this when <code>--use_env=False</code>, but if you are using per-process-launch you will need to explicitly pass this in as an argument to the training script <code>--local_rank=$LOCAL_RANK</code> as Azure ML only sets the LOCAL_RANK environment variable.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="tensorflow"></a>TensorFlow<a class="hash-link" href="#tensorflow" title="Direct link to heading">#</a></h2><p>If you are using <a href="https://www.tensorflow.org/guide/distributed_training" target="_blank" rel="noopener noreferrer">native distributed TensorFlow</a> in your training code, such as TensorFlow 2.x&#x27;s <code>tf.distribute.Strategy</code> API, you can launch the distributed job via Azure ML using the <code>TensorflowConfiguration</code>.</p><p>To do so, specify a <code>TensorflowConfiguration</code> object to the <code>distributed_job_config</code> parameter of the <code>ScriptRunConfig</code> constructor. If you are using <code>tf.distribute.experimental.MultiWorkerMirroredStrategy</code>, specify the <code>worker_count</code> in the <code>TensorflowConfiguration</code> corresponding to the number of nodes for your training job.</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly python"><div tabindex="0" class="prism-code language-python codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> azureml</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">core </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> ScriptRunConfig</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> Environment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> Experiment</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> azureml</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">core</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">runconfig </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> TensorflowConfiguration</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">curated_env_name </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;AzureML-TensorFlow-2.3-GPU&#x27;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">tf_env </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> Environment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">get</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">workspace</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">ws</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">curated_env_name</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">distr_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> TensorflowConfiguration</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">worker_count</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">2</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> parameter_server_count</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">0</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">run_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> ScriptRunConfig</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  source_directory</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;./src&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  script</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;train.py&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  compute_target</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">compute_target</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  environment</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">tf_env</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">  distributed_job_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">distr_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># submit the run configuration to start the job</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">run </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> Experiment</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">ws</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;experiment_name&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">submit</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">run_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><p>If your training script uses the parameter server strategy for distributed training, i.e. for legacy TensorFlow 1.x, you will also need to specify the number of parameter servers to use in the job, e.g. <code>tf_config = TensorflowConfiguration(worker_count=2, parameter_server_count=1)</code>.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="tf_config"></a>TF_CONFIG<a class="hash-link" href="#tf_config" title="Direct link to heading">#</a></h3><p>In TensorFlow, the <strong>TF_CONFIG</strong> environment variable is required for training on multiple machines. For TensorFlow jobs, Azure ML will configure and set the TF_CONFIG variable appropriately for each worker before executing your training script.</p><p>You can access TF_CONFIG from your training script if you need to: <code>os.environ[&#x27;TF_CONFIG&#x27;]</code>.</p><p>Example TF_CONFIG set on a chief worker node:</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly json"><div tabindex="0" class="prism-code language-json codeBlock_23N8 thin-scrollbar"><div class="codeBlockLines_39YC" style="color:#bfc7d5;background-color:#292d3e"><div class="token-line" style="color:#bfc7d5"><span class="token plain">TF_CONFIG=&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token property">&quot;cluster&quot;</span><span class="token operator" style="color:rgb(137, 221, 255)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token property">&quot;worker&quot;</span><span class="token operator" style="color:rgb(137, 221, 255)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;host0:2222&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;host1:2222&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token property">&quot;task&quot;</span><span class="token operator" style="color:rgb(137, 221, 255)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token property">&quot;type&quot;</span><span class="token operator" style="color:rgb(137, 221, 255)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;worker&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token property">&quot;index&quot;</span><span class="token operator" style="color:rgb(137, 221, 255)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">0</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token property">&quot;environment&quot;</span><span class="token operator" style="color:rgb(137, 221, 255)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;cloud&quot;</span><span class="token plain"></span></div><div class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain">&#x27;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o">Copy</button></div></div><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="example-5"></a>Example<a class="hash-link" href="#example-5" title="Direct link to heading">#</a></h4><ul><li><a href="https://github.com/Azure/azureml-examples/tree/main/workflows/train/tensorflow/mnist-distributed" target="_blank" rel="noopener noreferrer">azureml-examples: Distributed TensorFlow training with MultiWorkerMirroredStrategy</a></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="accelerating-gpu-training-with-infiniband"></a>Accelerating GPU training with InfiniBand<a class="hash-link" href="#accelerating-gpu-training-with-infiniband" title="Direct link to heading">#</a></h2><p>Certain Azure VM series, specifically the NC, ND, and H-series, now have RDMA-capable VMs with SR-IOV and Infiniband support. These VMs communicate over the low latency and high bandwidth InfiniBand network, which is much more performant than Ethernet-based connectivity. SR-IOV for InfiniBand enables near bare-metal performance for any MPI library (MPI is leveraged by many distributed training frameworks and tooling, including NVIDIA&#x27;s NCCL software.) These SKUs are intended to meet the needs of computationally-intensive, GPU-acclerated machine learning workloads. For more information, see <a href="https://techcommunity.microsoft.com/t5/azure-ai/accelerating-distributed-training-in-azure-machine-learning/ba-p/1059050" target="_blank" rel="noopener noreferrer">Accelerating Distributed Training in Azure Machine Learning with SR-IOV</a>.</p><p>If you create an <code>AmlCompute</code> cluster of one of these RDMA-capable, InfiniBand-enabled sizes, such as <code>Standard_ND40rs_v2</code>, the OS image will come with the Mellanox OFED driver required to enable InfiniBand preinstalled and preconfigured.</p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"><a href="https://github.com/Azure/azureml-cheatsheets/tree/main/website/docs/cheatsheets/python/v1/distributed-training.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="1.2em" width="1.2em" preserveAspectRatio="xMidYMid meet" role="img" viewBox="0 0 40 40" class="iconEdit_2_ui" aria-label="Edit page"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/logging"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Metrics</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/azureml-cheatsheets/docs/cheatsheets/python/v1/docker-build"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Azure ML Containers Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#basic-concepts" class="table-of-contents__link">Basic Concepts</a></li><li><a href="#mpi" class="table-of-contents__link">MPI</a><ul><li><a href="#horovod" class="table-of-contents__link">Horovod</a></li><li><a href="#deepspeed" class="table-of-contents__link">DeepSpeed</a></li><li><a href="#environment-variables-from-open-mpi" class="table-of-contents__link">Environment variables from Open MPI</a></li></ul></li><li><a href="#pytorch" class="table-of-contents__link">PyTorch</a><ul><li><a href="#process-group-initialization" class="table-of-contents__link">Process group initialization</a></li><li><a href="#launch-options" class="table-of-contents__link">Launch options</a></li><li><a href="#distributeddataparallel-per-process-launch" class="table-of-contents__link">DistributedDataParallel (per-process-launch)</a></li><li><a href="#using-torchdistributedlaunch-per-node-launch" class="table-of-contents__link">Using <code>torch.distributed.launch</code> (per-node-launch)</a></li><li><a href="#pytorch-lightning" class="table-of-contents__link">PyTorch Lightning</a></li><li><a href="#hugging-face-transformers" class="table-of-contents__link">Hugging Face Transformers</a></li></ul></li><li><a href="#tensorflow" class="table-of-contents__link">TensorFlow</a><ul><li><a href="#tf_config" class="table-of-contents__link">TF_CONFIG</a></li></ul></li><li><a href="#accelerating-gpu-training-with-infiniband" class="table-of-contents__link">Accelerating GPU training with InfiniBand</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><h4 class="footer__title">Resources</h4><ul class="footer__items"><li class="footer__item"><a href="https://docs.microsoft.com/azure/machine-learning" target="_blank" rel="noopener noreferrer" class="footer__link-item">Azure ML - Microsoft Docs</a></li><li class="footer__item"><a href="https://docs.microsoft.com/python/api/overview/azure/ml/?view=azure-ml-py" target="_blank" rel="noopener noreferrer" class="footer__link-item">Azure ML - Python API</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">Support</h4><ul class="footer__items"><li class="footer__item"><a href="https://github.com/Azure/azureml-cheatsheets/issues" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub issues</a></li><li class="footer__item"><a href="https://stackoverflow.microsoft.com/questions/tagged/10888" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow</a></li></ul></div><div class="col footer__col"><h4 class="footer__title">GitHub</h4><ul class="footer__items"><li class="footer__item"><a href="https://github.com/Azure/azureml-cheatsheets" target="_blank" rel="noopener noreferrer" class="footer__link-item">Cheat sheets</a></li><li class="footer__item"><a href="https://github.com/Azure/azureml-examples" target="_blank" rel="noopener noreferrer" class="footer__link-item">Azure ML Examples</a></li><li class="footer__item"><a class="footer__link-item" href="/azureml-cheatsheets/docs/misc/contributing">Contribution</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2021 Microsoft Corporation</div></div></div></footer></div>
<script src="/azureml-cheatsheets/assets/js/runtime~main.421f25ac.js"></script>
<script src="/azureml-cheatsheets/assets/js/main.ea6aa2e5.js"></script>
</body>
</html>