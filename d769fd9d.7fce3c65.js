(window.webpackJsonp=window.webpackJsonp||[]).push([[36],{105:function(e,n,t){"use strict";t.d(n,"a",(function(){return d})),t.d(n,"b",(function(){return m}));var r=t(0),a=t.n(r);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function c(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=a.a.createContext({}),p=function(e){var n=a.a.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},d=function(e){var n=p(e.components);return a.a.createElement(l.Provider,{value:n},e.children)},u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.a.createElement(a.a.Fragment,{},n)}},b=a.a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,i=e.originalType,o=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),d=p(t),b=r,m=d["".concat(o,".").concat(b)]||d[b]||u[b]||i;return t?a.a.createElement(m,s(s({ref:n},l),{},{components:t})):a.a.createElement(m,s({ref:n},l))}));function m(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=t.length,o=new Array(i);o[0]=b;var s={};for(var c in n)hasOwnProperty.call(n,c)&&(s[c]=n[c]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var l=2;l<i;l++)o[l]=t[l];return a.a.createElement.apply(null,o)}return a.a.createElement.apply(null,t)}b.displayName="MDXCreateElement"},94:function(e,n,t){"use strict";t.r(n),t.d(n,"frontMatter",(function(){return o})),t.d(n,"metadata",(function(){return s})),t.d(n,"rightToc",(function(){return c})),t.d(n,"default",(function(){return p}));var r=t(2),a=t(6),i=(t(0),t(105)),o={title:"Distributed GPU Training",id:"distributed-training",description:"Guide to distributed training in Azure ML.",keywords:["distributed training","mpi","process group"]},s={unversionedId:"cheatsheet/distributed-training",id:"cheatsheet/distributed-training",isDocsHomePage:!1,title:"Distributed GPU Training",description:"Guide to distributed training in Azure ML.",source:"@site/docs/cheatsheet/distributed-training.md",slug:"/cheatsheet/distributed-training",permalink:"/azureml-web/docs/cheatsheet/distributed-training",editUrl:"https://github.com/Azure/azureml-web/tree/main/website/docs/cheatsheet/distributed-training.md",version:"current",sidebar:"mainSidebar",previous:{title:"Metrics",permalink:"/azureml-web/docs/cheatsheet/logging"},next:{title:"Azure ML Containers",permalink:"/azureml-web/docs/cheatsheet/docker-build"}},c=[{value:"Basic Concepts",id:"basic-concepts",children:[{value:"Process Group and Communication Backend",id:"process-group-and-communication-backend",children:[]},{value:"Launch Distributed Training",id:"launch-distributed-training",children:[]}]},{value:"AzureML Distributed Learning Utilities",id:"azureml-distributed-learning-utilities",children:[{value:"AzureML MPIRUN",id:"azureml-mpirun",children:[]},{value:"Environment Variables from OpenMPI",id:"environment-variables-from-openmpi",children:[]}]},{value:"Accelerating GPU training with InfiniBand",id:"accelerating-gpu-training-with-infiniband",children:[]},{value:"Examples",id:"examples",children:[{value:"PyTorch Distributed Data Parallel (Per-Processes-Launch)",id:"pytorch-distributed-data-parallel-per-processes-launch",children:[]},{value:"PyTorch <code>torch.distributed.launch</code> (Per-Node-Launch)",id:"pytorch-torchdistributedlaunch-per-node-launch",children:[]},{value:"HuggingFace Transformer Trainer",id:"huggingface-transformer-trainer",children:[]},{value:"PyTorch Lightning ddp accelerator (Per-Node-Launch)",id:"pytorch-lightning-ddp-accelerator-per-node-launch",children:[]}]},{value:"TensorFlow",id:"tensorflow",children:[]}],l={rightToc:c};function p(e){var n=e.components,t=Object(a.a)(e,["components"]);return Object(i.b)("wrapper",Object(r.a)({},l,t,{components:n,mdxType:"MDXLayout"}),Object(i.b)("h2",{id:"basic-concepts"},"Basic Concepts"),Object(i.b)("p",null,"We assume readers already understand the basic concept of distributed GPU training such as ",Object(i.b)("em",{parentName:"p"},"data parallelism, distributed data parallelism, and model parallelism"),". This guide aims at helping readers running existing distributed training code on AzureML. "),Object(i.b)("div",{className:"admonition admonition-info alert alert--info"},Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(r.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(r.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(r.a)({parentName:"svg"},{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"})))),"info")),Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"Most of this guide is based on ",Object(i.b)("strong",{parentName:"p"},"PyTorch")," and packages on top of it. For ",Object(i.b)("strong",{parentName:"p"},"TensorFlow"),", see ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"#tensorflow"}),"TensorFlow"),"."),Object(i.b)("p",{parentName:"div"},"If you don't know which type of parallelism to use, for >90% of the time you should use ",Object(i.b)("strong",{parentName:"p"},"Distributed Data Parallelism"),"."))),Object(i.b)("h3",{id:"process-group-and-communication-backend"},"Process Group and Communication Backend"),Object(i.b)("p",null,"The backbone of any distributed training is based on a group of processes that knows each other and\ncan communicate with each other using a backend. For PyTorch, the process group is created by calling ",Object(i.b)("inlineCode",{parentName:"p"},"torch.distributed.init_process_group")," in ",Object(i.b)("strong",{parentName:"p"},"all distributed processes")," to collectively form a process group. "),Object(i.b)("pre",null,Object(i.b)("code",Object(r.a)({parentName:"pre"},{}),"torch.distributed.init_process_group(backend='nccl', init_method='env://', ...)\n")),Object(i.b)("p",null,"The most common backend used are ",Object(i.b)("strong",{parentName:"p"},"mpi"),", ",Object(i.b)("strong",{parentName:"p"},"nccl")," and ",Object(i.b)("strong",{parentName:"p"},"gloo"),". For GPU based training ",Object(i.b)("strong",{parentName:"p"},"nccl")," is strongly preferred and should be used whenever possible. ",Object(i.b)("inlineCode",{parentName:"p"},"Init_method")," specifies how each processes can discover each other and initialize as well as verify the process group using the communication backend. By default PyTorch will look for environment variables. The following is a list of key environment variables, documented ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group"}),"here"),". "),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"MASTER_PORT - required; has to be a free port on machine with rank 0"),Object(i.b)("li",{parentName:"ul"},"MASTER_ADDR - required (except for rank 0); address of rank 0 node"),Object(i.b)("li",{parentName:"ul"},"WORLD_SIZE - required; can be set either here, or in a call to init function. The total number of processes. This should be equal to the number of devices (GPU) used for distributed training. "),Object(i.b)("li",{parentName:"ul"},"RANK - required; can be set either here, or in a call to init function. The rank (0 to world_size - 1) of the current process in the process group. ")),Object(i.b)("p",null,"Beyond these, many application will need the following "),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"LOCAL_RANK - the relative rank within the node. This information is useful because many operations such as data preparation only should be performed once per node --- usually on local_rank = 0."),Object(i.b)("li",{parentName:"ul"},"NODE_RANK - the relative rank for the node among all the nodes. ")),Object(i.b)("div",{className:"admonition admonition-info alert alert--info"},Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(r.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(r.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(r.a)({parentName:"svg"},{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"})))),"info")),Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"RANK can be inferred by NODE_RANK and LOCAL_RANK. NODE_RANK is often used by utility launcher script (such as ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"https://github.com/pytorch/pytorch/blob/master/torch/distributed/launch.py"}),"torch.distributed.launch"),") that can created multiple processes on the same node. See ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"#launch-processes-in-distributed-nodes"}),"Launch Processes in Distributed Nodes"),". "),Object(i.b)("p",{parentName:"div"},"LOCAL_RANK and RANK are both process level environment variables which are not set for the node but for the process. "))),Object(i.b)("h3",{id:"launch-distributed-training"},"Launch Distributed Training"),Object(i.b)("p",null,"Users rarely launch all distributed processes manually and often rely on a utility launcher. There are 3 types of utility launchers. "),Object(i.b)("ol",null,Object(i.b)("li",{parentName:"ol"},Object(i.b)("strong",{parentName:"li"},"Per-process-launcher"),": the system will launch all distributed processes for users, with all the relevant information (e.g. environment variables) to set up process groups. "),Object(i.b)("li",{parentName:"ol"},Object(i.b)("strong",{parentName:"li"},"Per-node-launcher"),": the utility launcher will launch processes on a given node. User is responsible to run the launcher from multiple nodes and provide global information such as WORLD_SIZE and MASTER_ADDR, MASTER_ADDR. Locally within each node, RANK and LOCAL_RANK is set up by the launcher, with user provided NODE_RANK. ",Object(i.b)("inlineCode",{parentName:"li"},"torch.distributed.launch")," belongs to this category, as well as ",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://pytorch-lightning.readthedocs.io/en/stable/trainer.html#accelerator"}),"pytorch-lightning Trainer using ddp accelerator"),". "),Object(i.b)("li",{parentName:"ol"},Object(i.b)("strong",{parentName:"li"},"Head-node-launcher"),": User run launcher at the head provide information about the cluster (e.g. a hostfile) and launcher arguments, the training script and arguments for the training script. Three examples of head node launcher are ",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://www.open-mpi.org/doc/v4.0/man1/mpirun.1.php"}),"mpirun"),", ",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://www.deepspeed.ai/getting-started/#launching-deepspeed-training"}),"DeepSpeed launcher")," and ",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://horovod.readthedocs.io/en/stable/running_include.html"}),"Horovodrun"),".")),Object(i.b)("p",null,"This three categories of launchers are named with respect to user experience. Per-process-launcher means user does not need to do extra launching effort, per-node-launcher means user need to be able to run launcher script on every node, and head-node-launcher requires user to get on a headnode with cluster information usually in a hostfile. There are no fundamental differences between the three types of launchers and eventually what matters is the process group getting initiated with the proper backend. Behind the scene a head-node-launcher is often used on behalf of the user by the system so user are exposed to a per-process-launcher experience. Head-node-launcher is often implemented as a wrapper of per-node-launcher. "),Object(i.b)("h2",{id:"azureml-distributed-learning-utilities"},"AzureML Distributed Learning Utilities"),Object(i.b)("h3",{id:"azureml-mpirun"},"AzureML MPIRUN"),Object(i.b)("p",null,"AzureML supports mpirun to launch a given number of processes in each node. User can adopt this approach to run distributed training using either per-process-launcher or per-node-launcher, depending on whether user set ",Object(i.b)("inlineCode",{parentName:"p"},"process_count_per_node")," to be only 1 for per-node-launcher, or equal to the number of devices/GPUs for per-process-launcher. Currently AzureML does not expose cluster hostfiles for user to launch a head-node-launcher like DeepSpeed launcher. It runs ",Object(i.b)("inlineCode",{parentName:"p"},"mpirun")," behind the scene."),Object(i.b)("p",null,"No matter which launch style user choose, users can follow these steps:"),Object(i.b)("ol",null,Object(i.b)("li",{parentName:"ol"},"Use an AzureML environment with the preferred deep learning framework and MPI. AzureML provides ",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments"}),"curated environment")," for popular frameworks."),Object(i.b)("li",{parentName:"ol"},"Define ",Object(i.b)("inlineCode",{parentName:"li"},"MpiConfiguration")," with the desired ",Object(i.b)("inlineCode",{parentName:"li"},"process_per_node")," and ",Object(i.b)("inlineCode",{parentName:"li"},"node_count"),". ",Object(i.b)("inlineCode",{parentName:"li"},"process_per_node")," should be equal to the number of GPUs per node for per-process-launch, or set to 1 for per-node-launch if user script will be responsible to launch processes for each node."),Object(i.b)("li",{parentName:"ol"},"Set the ",Object(i.b)("inlineCode",{parentName:"li"},"mpi")," section of the ",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfig.runconfiguration?view=azure-ml-py"}),"runconfig")," using the ",Object(i.b)("inlineCode",{parentName:"li"},"MpiConfiguration"),". When using ",Object(i.b)("inlineCode",{parentName:"li"},"ScriptRunConfig")," simply pass it as the value for argument ",Object(i.b)("inlineCode",{parentName:"li"},"distributed_job_config"),"."),Object(i.b)("li",{parentName:"ol"},"Prepare the user script and map environment variables AzureML set up to your needs. See ",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"#environment-variables-from-openmpi"}),"detailed instruction")," and ",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"#examples"}),"examples"),".")),Object(i.b)("p",null,"Here is a code snippet:"),Object(i.b)("pre",null,Object(i.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"from azureml.core import Workspace, ScriptRunConfig, Environment\nfrom azureml.core.runconfig import MpiConfiguration\n\ncurated_env_name = 'AzureML-PyTorch-1.6-GPU'\npytorch_env = Environment.get(workspace=ws, name=curated_env_name)\nmpiconfig = MpiConfiguration(process_count_per_node=4, node_count=2)\n\nrun_cf = ScriptRunConfig(\n    source_directory= 'src' ,\n    script=\"train.py\",\n    compute_target=compute_target,\n    distributed_job_config=mpiconfig,\n    environment=pytorch_env \n)\n\n# submit the runconfig and run the job\nexperiment = Experiment(ws, \"experiment_name\")\nexperiment.submit(run_cf)\n")),Object(i.b)("div",{className:"admonition admonition-caution alert alert--warning"},Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(r.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(r.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"}),Object(i.b)("path",Object(r.a)({parentName:"svg"},{fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"})))),"caution")),Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"To use AzureML mpirun, the base docker image used by the run need to have Open MPI. They are included in ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"https://github.com/Azure/AzureML-Containers"}),"all AzureML default GPU base images"),". User is responsible to\ninstall one of these when using custom base image. AzureML also provides ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments"}),"curated environment")," for popular frameworks. "))),Object(i.b)("div",{className:"admonition admonition-tip alert alert--success"},Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(r.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(r.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"}),Object(i.b)("path",Object(r.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"})))),"tip")),Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"It is a common confusion of the concept of mpirun and mpi as communication backend and users might think AzureML mpirun won't be able to run distributed jobs with ",Object(i.b)("inlineCode",{parentName:"p"},"nccl")," or ",Object(i.b)("inlineCode",{parentName:"p"},"gloo")," backend. Unfortunately the difference is not well explained in official AzureML documentation. AzureML use mpirun as launcher utility to launch processes for user's training script in distributed nodes and sets up all necessary environment variables which user can use to init the process group. The real communication backend is what user set in their ",Object(i.b)("inlineCode",{parentName:"p"},"init_process_group")," call. "))),Object(i.b)("h3",{id:"environment-variables-from-openmpi"},"Environment Variables from OpenMPI"),Object(i.b)("p",null,"When running MPIRUN with OpenMPI images, AzureML set the following environment variables for each process launched:"),Object(i.b)("ol",null,Object(i.b)("li",{parentName:"ol"},"OMPI_COMM_WORLD_RANK - the rank of the process"),Object(i.b)("li",{parentName:"ol"},"OMPI_COMM_WORLD_SIZE - the world size"),Object(i.b)("li",{parentName:"ol"},"AZ_BATCH_MASTER_NODE - master address with port, MASTER_ADDR:MASTER_PORT"),Object(i.b)("li",{parentName:"ol"},"OMPI_COMM_WORLD_LOCAL_RANK - the local rank of the process on the node"),Object(i.b)("li",{parentName:"ol"},"OMPI_COMM_WORLD_LOCAL_SIZE - number of processes on the node")),Object(i.b)("div",{className:"admonition admonition-caution alert alert--warning"},Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(r.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(r.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"}),Object(i.b)("path",Object(r.a)({parentName:"svg"},{fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"})))),"caution")),Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"Despite the name, environment variable OMPI_COMM_WORLD_NODE_RANK does not corresponds to the NODE_RANK. To use per-node-launcher, simply set ",Object(i.b)("inlineCode",{parentName:"p"},"process_count_per_node=1")," and use ",Object(i.b)("inlineCode",{parentName:"p"},"OMPI_COMM_WORLD_RANK")," as the NODE_RANK. "))),Object(i.b)("p",null,"The following code maps the OpenMPI environment variables to PyTorch style. For majority of the pytorch script, simply call ",Object(i.b)("inlineCode",{parentName:"p"},"set_environment_variables_for_nccl_backend()")," function before your script calls ",Object(i.b)("inlineCode",{parentName:"p"},"torch.distributed.init_process_group"),". If your script passes in information like local_rank or rank as script arguments, just remove these and use provided helper functions ",Object(i.b)("inlineCode",{parentName:"p"},"get_local_rank()")," and ",Object(i.b)("inlineCode",{parentName:"p"},"get_rank()")," instead."),Object(i.b)("pre",null,Object(i.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python",metastring:'title="aml_mpienv.py"',title:'"aml_mpienv.py"'}),'import os\n\ndef set_environment_variables_for_nccl_backend(master_port=6105, verbose=True):\n    os.environ["RANK"] = os.environ["OMPI_COMM_WORLD_RANK"]\n    os.environ["WORLD_SIZE"] = os.environ["OMPI_COMM_WORLD_SIZE"]\n    single_node = int(os.environ["OMPI_COMM_WORLD_LOCAL_SIZE"]) == int(\n        os.environ["WORLD_SIZE"]\n    )\n    if not single_node:\n        master_node_params = os.environ["AZ_BATCH_MASTER_NODE"].split(":")\n        os.environ["MASTER_ADDR"] = master_node_params[0]\n        # Do not overwrite master port with that defined in AZ_BATCH_MASTER_NODE\n        if "MASTER_PORT" not in os.environ:\n            os.environ["MASTER_PORT"] = str(master_port)\n    else:\n        os.environ["MASTER_ADDR"] = os.environ["AZ_BATCHAI_MPI_MASTER_NODE"]\n        os.environ["MASTER_PORT"] = "54965"\n    print(\n        "NCCL_SOCKET_IFNAME original value = {}".format(\n            os.environ["NCCL_SOCKET_IFNAME"]\n        )\n    )\n    os.environ["NCCL_SOCKET_IFNAME"] = "^docker0,lo"\n    if verbose:\n        print("RANK = {}".format(os.environ["RANK"]))\n        print("WORLD_SIZE = {}".format(os.environ["WORLD_SIZE"]))\n        print("MASTER_ADDR = {}".format(os.environ["MASTER_ADDR"]))\n        print("MASTER_PORT = {}".format(os.environ["MASTER_PORT"]))\n        print(\n            "NCCL_SOCKET_IFNAME new value = {}".format(os.environ["NCCL_SOCKET_IFNAME"])\n        )\n\ndef get_rank():\n    return int(os.environ["OMPI_COMM_WORLD_RANK"])\n\ndef get_local_rank():\n    return int(os.environ["OMPI_COMM_WORLD_LOCAL_RANK"])\n\ndef get_global_size():\n    return int(os.environ["OMPI_COMM_WORLD_SIZE"])\n\ndef get_local_size():\n    return int(os.environ["OMPI_COMM_WORLD_LOCAL_SIZE"])\n\ndef get_world_size():\n    return int(os.environ["OMPI_COMM_WORLD_SIZE"])\n')),Object(i.b)("div",{className:"admonition admonition-tip alert alert--success"},Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(r.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(r.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"}),Object(i.b)("path",Object(r.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"})))),"tip")),Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"To see the list of environment variables provided by AzureML, just print ",Object(i.b)("inlineCode",{parentName:"p"},"os.environ")," in your training script.  "),Object(i.b)("pre",{parentName:"div"},Object(i.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"import os\nprint(os.environ)\n")))),Object(i.b)("h2",{id:"accelerating-gpu-training-with-infiniband"},"Accelerating GPU training with InfiniBand"),Object(i.b)("p",null,"Certain Azure VM series, specifically the NC, ND, and H-series, now have RDMA-capable VMs with SR-IOV and Infiniband support. These VMs communicate over the low latency and high bandwidth InfiniBand network, which is much more performant than Ethernet-based connectivity. SR-IOV for InfiniBand enables near bare-metal performance for any MPI library (MPI is leveraged by many distributed training frameworks and tooling, including NVIDIA's NCCL software.) These SKUs are intended to meet the needs of computationally-intensive, GPU-acclerated machine learning workloads. For more information, see ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"https://techcommunity.microsoft.com/t5/azure-ai/accelerating-distributed-training-in-azure-machine-learning/ba-p/1059050"}),"Accelerating Distributed Training in Azure Machine Learning with SR-IOV"),"."),Object(i.b)("p",null,"If you create an ",Object(i.b)("inlineCode",{parentName:"p"},"AmlCompute")," cluster of one of these RDMA-capable, InfiniBand-enabled sizes, such as ",Object(i.b)("inlineCode",{parentName:"p"},"Standard_ND40rs_v2"),", the OS image will come with the Mellanox OFED driver required to enable InfiniBand preinstalled and preconfigured."),Object(i.b)("h2",{id:"examples"},"Examples"),Object(i.b)("h3",{id:"pytorch-distributed-data-parallel-per-processes-launch"},"PyTorch Distributed Data Parallel (Per-Processes-Launch)"),Object(i.b)("p",null,"For majority of cases, users can use AzureML MPIRun to launch scripts for each processes without resorting to any launcher utilities like ",Object(i.b)("inlineCode",{parentName:"p"},"torch.distributed.launch"),", following three steps:"),Object(i.b)("ol",null,Object(i.b)("li",{parentName:"ol"},"Put ",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"#environment-variables-from-openmpi"}),Object(i.b)("inlineCode",{parentName:"a"},"aml_mpienv.py"))," in the source directory so it can be imported by the launcher script."),Object(i.b)("li",{parentName:"ol"},"Call the ",Object(i.b)("inlineCode",{parentName:"li"},"set_environment_variables_for_nccl_backend")," function before ",Object(i.b)("inlineCode",{parentName:"li"},"init_process_group")," call."),Object(i.b)("li",{parentName:"ol"},"Use LOCAL_RANK environment variable to set the GPU device for the process. Use ",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"#environment-variables-from-openmpi"}),"other environment variables")," when needed.")),Object(i.b)("pre",null,Object(i.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),'from azureml.core import ScriptRunConfig, Environment\nfrom azureml.core.runconfig import MpiConfiguration\n\ncurated_env_name = \'AzureML-PyTorch-1.6-GPU\'\npytorch_env = Environment.get(workspace=ws, name=curated_env_name)\nmpiconfig = MpiConfiguration(process_count_per_node=4, node_count=2)\nrun = ScriptRunConfig(\n    source_directory=...,\n    script="train.py",\n    arguments=...,\n    compute_target=...,\n    distributed_job_config=mpiconfig,\n    environment=pytorch_env,\n)\n\nexperiment = Experiment(ws, "pt_dist_launch")\nexperiment.submit(run)\n')),Object(i.b)("pre",null,Object(i.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python",metastring:'title="train.py"',title:'"train.py"'}),'from aml_mpienv import set_environment_variables_for_nccl_backend, get_local_rank\n\nset_environment_variables_for_nccl_backend()\ntorch.distributed.init_process_group(backend="nccl")\nlocal_rank = get_local_rank()\n\nimport torch.distributed as dist\n\nif dist.is_initialized():\n    model = torch.nn.parallel.DistributedDataParallel(\n        model,\n        device_ids=[local_rank],\n        output_device=local_rank,\n        find_unused_parameters=True,\n    )\n')),Object(i.b)("h3",{id:"pytorch-torchdistributedlaunch-per-node-launch"},"PyTorch ",Object(i.b)("inlineCode",{parentName:"h3"},"torch.distributed.launch")," (Per-Node-Launch)"),Object(i.b)("p",null,"We use ",Object(i.b)("inlineCode",{parentName:"p"},"torch.distributed.launch")," as an example to show how to do per-node-launch using AzureML. For users already with script compatible with ",Object(i.b)("inlineCode",{parentName:"p"},"torch.distributed.launch"),", here are key steps:"),Object(i.b)("ol",null,Object(i.b)("li",{parentName:"ol"},"Put ",Object(i.b)("inlineCode",{parentName:"li"},"aml_mpienv.py")," in the source directory so it can be imported by the launcher script."),Object(i.b)("li",{parentName:"ol"},"Use the customized ",Object(i.b)("a",Object(r.a)({parentName:"li"},{href:"#launchpy"}),Object(i.b)("inlineCode",{parentName:"a"},"launch.py"))," as the launcher script. This is the original ",Object(i.b)("inlineCode",{parentName:"li"},"torch.distributed.launch")," (code shown below) to use AML's OpenMPI environment variables."),Object(i.b)("li",{parentName:"ol"},"Set ",Object(i.b)("inlineCode",{parentName:"li"},"process_count_per_node=1")," in AzureML's ",Object(i.b)("inlineCode",{parentName:"li"},"MpiConfiguration"),", but use argument ",Object(i.b)("inlineCode",{parentName:"li"},"--nproc_per_node")," for the number of processes per node to the launch.py, and pass the training script as argument to the launch.py similar to ",Object(i.b)("inlineCode",{parentName:"li"},"torch.distributed.launch"),". ")),Object(i.b)("pre",null,Object(i.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),'from azureml.core import ScriptRunConfig\nfrom azureml.core.runconfig import MpiConfiguration\n\nmpiconfig = MpiConfiguration(process_count_per_node=1, node_count=2)\n\nrun = ScriptRunConfig(\n    source_directory=...,\n    script="launch.py",\n    arguments=["--nproc_per_node", 4, "--use_env", "example.py"],\n    compute_target=compute_target,\n    distributed_job_config=mpiconfig,\n    environment=pytorch_env,\n)\n\nexperiment = Experiment(ws, "pt_dist_launch")\nexperiment.submit(run)\n')),Object(i.b)("pre",null,Object(i.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python",metastring:'title="launch.py"',title:'"launch.py"'}),'#####################################################################################\n# launch.py\n# based on https://github.com/pytorch/pytorch/blob/master/torch/distributed/launch.py\n#####################################################################################\nimport sys\nimport subprocess\nimport os\nfrom argparse import ArgumentParser, REMAINDER\n\nfrom aml_mpienv import set_environment_variables_for_nccl_backend\n\ndef parse_args():\n    """\n    Helper function parsing the command line options\n    @retval ArgumentParser\n    """\n    parser = ArgumentParser(\n        description="PyTorch distributed training launch modified to use AML OpenMPI Env Vars"\n        "helper utility that will spawn up "\n        "multiple distributed processes"\n    )\n\n    # Optional arguments for the launch helper\n\n    parser.add_argument(\n        "--nproc_per_node",\n        type=int,\n        default=1,\n        help="The number of processes to launch on each node, "\n        "for GPU training, this is recommended to be set "\n        "to the number of GPUs in your system so that "\n        "each process can be bound to a single GPU.",\n    )\n    parser.add_argument(\n        "--use_env",\n        default=False,\n        action="store_true",\n        help="Use environment variable to pass "\n        "\'local rank\'. For legacy reasons, the default value is False. "\n        "If set to True, the script will not pass "\n        "--local_rank as argument, and will instead set LOCAL_RANK.",\n    )\n    parser.add_argument(\n        "-m",\n        "--module",\n        default=False,\n        action="store_true",\n        help="Changes each process to interpret the launch script "\n        "as a python module, executing with the same behavior as"\n        "\'python -m\'.",\n    )\n    parser.add_argument(\n        "--no_python",\n        default=False,\n        action="store_true",\n        help=\'Do not prepend the training script with "python" - just exec \'\n        "it directly. Useful when the script is not a Python script.",\n    )\n\n    # positional\n    parser.add_argument(\n        "training_script",\n        type=str,\n        help="The full path to the single GPU training "\n        "program/script to be launched in parallel, "\n        "followed by all the arguments for the "\n        "training script",\n    )\n\n    # rest from the training program\n    parser.add_argument("training_script_args", nargs=REMAINDER)\n    return parser.parse_args()\n\ndef main():\n    print(\n        "Important! When using this launcher, make sure number of processes per node when launching AML job is 1. Using nproc_per_node argument to pass in number of processes per node!"\n    )\n    set_environment_variables_for_nccl_backend(verbose=False)\n    args = parse_args()\n    args.nnodes = int(\n        os.environ["WORLD_SIZE"]\n    )  # workd_size equals number of nodes when process per node is 1\n    args.node_rank = int(os.environ["RANK"])  # node_rank equals RANK\n    print(f"Number of nodes: {args.nnodes}.")\n    print(f"NODE_RANK: {args.node_rank}.")\n    # world size in terms of number of processes\n    dist_world_size = args.nproc_per_node * args.nnodes\n\n    # set PyTorch distributed related environmental variables\n    current_env = os.environ.copy()\n    current_env["WORLD_SIZE"] = str(dist_world_size)\n    print(f\'Reset WORLD_SIZE to {current_env["WORLD_SIZE"]}.\')\n\n    processes = []\n\n    if "OMP_NUM_THREADS" not in os.environ and args.nproc_per_node > 1:\n        current_env["OMP_NUM_THREADS"] = str(1)\n        print(\n            "*****************************************\\n"\n            "Setting OMP_NUM_THREADS environment variable for each process "\n            "to be {} in default, to avoid your system being overloaded, "\n            "please further tune the variable for optimal performance in "\n            "your application as needed. \\n"\n            "*****************************************".format(\n                current_env["OMP_NUM_THREADS"]\n            )\n        )\n\n    for local_rank in range(0, args.nproc_per_node):\n        # each process\'s rank\n        dist_rank = args.nproc_per_node * args.node_rank + local_rank\n        current_env["RANK"] = str(dist_rank)\n        current_env["LOCAL_RANK"] = str(local_rank)\n        print(f"RANK: {dist_rank}")\n        print(f"LOCAL_RANK: {local_rank}")\n        # spawn the processes\n        with_python = not args.no_python\n        cmd = []\n        if with_python:\n            cmd = [sys.executable, "-u"]\n            if args.module:\n                cmd.append("-m")\n        else:\n            if not args.use_env:\n                raise ValueError(\n                    "When using the \'--no_python\' flag, you must also set the \'--use_env\' flag."\n                )\n            if args.module:\n                raise ValueError(\n                    "Don\'t use both the \'--no_python\' flag and the \'--module\' flag at the same time."\n                )\n\n        cmd.append(args.training_script)\n\n        if not args.use_env:\n            cmd.append("--local_rank={}".format(local_rank))\n\n        cmd.extend(args.training_script_args)\n\n        process = subprocess.Popen(cmd, env=current_env)\n        processes.append(process)\n\n    for process in processes:\n        process.wait()\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n\nif __name__ == "__main__":\n    main()\n')),Object(i.b)("pre",null,Object(i.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python",metastring:'title="example.py"',title:'"example.py"'}),'# example.py \nimport argparse\nimport os\nimport torch\nfrom argparse import ArgumentParser, REMAINDER\n\nparser = ArgumentParser("testing")\nargs = parser.parse_args()\n\ntorch.distributed.init_process_group("nccl")\nargs.local_rank = os.environ["LOCAL_RANK"]\ncuda = torch.device(f"cuda:{args.local_rank}")\ntestTensor = torch.tensor(\n    [1], device=cuda\n)  # each with the device index equal to the local rank\ntorch.distributed.all_reduce(testTensor)\nprint(testTensor)\n')),Object(i.b)("h3",{id:"huggingface-transformer-trainer"},"HuggingFace Transformer Trainer"),Object(i.b)("h4",{id:"per-node-launch-using-launchpy"},"Per-Node-Launch using launch.py"),Object(i.b)("p",null,"Huggingface provides ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"https://github.com/huggingface/transformers/tree/master/examples/"}),"many examples")," using ",Object(i.b)("inlineCode",{parentName:"p"},"torch.distributed.launch"),". Follow the ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"#pytorch-torchdistributedlaunch-per-node-launch"}),"launch.py")," guide to run these code without without changing the training script. "),Object(i.b)("h4",{id:"per-processes-launch"},"Per-Processes-Launch"),Object(i.b)("p",null,"To launch training script directly without launch.py, remember the key is to set up environment using ",Object(i.b)("inlineCode",{parentName:"p"},"set_environment_variables_for_nccl_backend")," function before ",Object(i.b)("inlineCode",{parentName:"p"},"init_process_group")," call. Huggingface transformer sets the process group in its ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"https://huggingface.co/transformers/main_classes/trainer.html?highlight=launch#trainingarguments"}),"TrainingArguments"),", which also expects ",Object(i.b)("inlineCode",{parentName:"p"},"local_rank")," to be passed in as argument (which is provided by ",Object(i.b)("inlineCode",{parentName:"p"},"torch.distributed.launch")," when ",Object(i.b)("inlineCode",{parentName:"p"},"use_env=False"),"). In fact, the ",Object(i.b)("inlineCode",{parentName:"p"},"init_process_group")," call happens when ",Object(i.b)("inlineCode",{parentName:"p"},".gpu")," or ",Object(i.b)("inlineCode",{parentName:"p"},".device")," property is called on the TrainingArguments object. The following code snippet is adapted from the ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py"}),"GLUE example"),"."),Object(i.b)("div",{className:"admonition admonition-info alert alert--info"},Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(r.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(r.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(r.a)({parentName:"svg"},{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"})))),"info")),Object(i.b)("div",Object(r.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"Make sure to call ",Object(i.b)("inlineCode",{parentName:"p"},"set_environment_variables_for_nccl_backend")," and set ",Object(i.b)("inlineCode",{parentName:"p"},"local_rank")," ",Object(i.b)("strong",{parentName:"p"},"right after the TrainingArguments object is created"),". "))),Object(i.b)("pre",null,Object(i.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python",metastring:'title="run_glue.py"',title:'"run_glue.py"'}),'#####################################################################################################\n## https://github.com/huggingface/transformers/blob/master/examples/text-classification/run_glue.py\n#####################################################################################################\ndef main():\n    # See all possible arguments in src/transformers/training_args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n\n    parser = HfArgumentParser(\n        (ModelArguments, DataTrainingArguments, TrainingArguments)\n    )\n\n    if len(sys.argv) == 2 and sys.argv[1].endswith(".json"):\n        # If we pass only one argument to the script and it\'s the path to a json file,\n        # let\'s parse it to get our arguments.\n        model_args, data_args, training_args = parser.parse_json_file(\n            json_file=os.path.abspath(sys.argv[1])\n        )\n    else:\n        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n    if (\n        os.path.exists(training_args.output_dir)\n        and os.listdir(training_args.output_dir)\n        and training_args.do_train\n        and not training_args.overwrite_output_dir\n    ):\n        raise ValueError(\n            f"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome."\n        )\n    # set distributed learning env var and local_rank. The first time training_args.device is called, it will init the process group\n    set_environment_variables_for_nccl_backend()\n    local_rank = get_local_rank()\n    training_args.local_rank = local_rank\n    ..........\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=build_compute_metrics_fn(data_args.task_name),\n    )\n')),Object(i.b)("h3",{id:"pytorch-lightning-ddp-accelerator-per-node-launch"},"PyTorch Lightning ddp accelerator (Per-Node-Launch)"),Object(i.b)("p",null,Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"https://pytorch-lightning.readthedocs.io/en/stable/"}),"PyTorch Lightning")," takes a great care to make your script run in single GPU, single machine multiple GPU and also multiple nodes multiple GPU. Behind the scene it launches\nmultiple processes for user similar to ",Object(i.b)("inlineCode",{parentName:"p"},"torch.distributed.launch"),". To use the default ddp accelerator when ",Object(i.b)("inlineCode",{parentName:"p"},"num_nodes")," is set to be more than 1, we can largely follow the ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"#pytorch-torchdistributedlaunch-per-node-launch"}),"per-node-launch guide"),". Instead of using ",Object(i.b)("inlineCode",{parentName:"p"},"launch.py")," --- pytorch-lightning will do that for you---, simply prepare the following environment variables:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"MASTER_ADDR"),Object(i.b)("li",{parentName:"ul"},"MASTER_PORT"),Object(i.b)("li",{parentName:"ul"},"NODE_RANK")),Object(i.b)("p",null,"That's right, pytorch-lightning will compute the world size from Trainer flags ",Object(i.b)("inlineCode",{parentName:"p"},"num_nodes")," and ",Object(i.b)("inlineCode",{parentName:"p"},"gpus")," and manage RANK and LOCAL_RANK using its own internal processes launching steps. Put the following snippets right after the script arguments are parsed and it is all set!"),Object(i.b)("pre",null,Object(i.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),'import os\nsingle_node = args.num_nodes==1\nif not single_node:\n    master_node_params = os.environ["AZ_BATCH_MASTER_NODE"].split(":")\n    os.environ["MASTER_ADDR"] = master_node_params[0]\n    # Do not overwrite master port with that defined in AZ_BATCH_MASTER_NODE\n    if "MASTER_PORT" not in os.environ:\n        os.environ["MASTER_PORT"] = str(master_port)\nelse:\n    os.environ["MASTER_ADDR"] = os.environ["AZ_BATCHAI_MPI_MASTER_NODE"]\n    os.environ["MASTER_PORT"] = "54965"\n\nos.environ["NCCL_SOCKET_IFNAME"] = "^docker0,lo"\nos.environ["NODE_RANK"] = os.environ["OMPI_COMM_WORLD_RANK"] \n')),Object(i.b)("pre",null,Object(i.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"## create trainer from arguments and \ntrainer: Trainer = pl.Trainer.from_argparse_args(\n    args, logger = [mylogger], ...\n)\n\ntrainer.fit(model, train_loader, val_loader)\n")),Object(i.b)("h2",{id:"tensorflow"},"TensorFlow"),Object(i.b)("p",null,"When a ",Object(i.b)("inlineCode",{parentName:"p"},"TensorflowConfiguration")," is set to be the distributed_job_config parameter of the ScriptRunConfig constructor, AzureML sets up environment variable ",Object(i.b)("inlineCode",{parentName:"p"},"TF_CONFIG")," in all nodes for ",Object(i.b)("a",Object(r.a)({parentName:"p"},{href:"https://www.tensorflow.org/guide/distributed_training"}),"native distributed TensorFlow")," API ",Object(i.b)("inlineCode",{parentName:"p"},"tf.distribute.Strategy"),". If you are using ",Object(i.b)("inlineCode",{parentName:"p"},"tf.distribute.experimental.MultiWorkerMirroredStrategy"),", specify the worker_count in the ",Object(i.b)("inlineCode",{parentName:"p"},"TensorflowConfiguration")," corresponding to the number of nodes for your training job."),Object(i.b)("pre",null,Object(i.b)("code",Object(r.a)({parentName:"pre"},{className:"language-python"}),"from azureml.core import ScriptRunConfig\nfrom azureml.core.runconfig import TensorflowConfiguration\n\ndistr_config = TensorflowConfiguration(worker_count=2, parameter_server_count=0)\n\nsrc = ScriptRunConfig(source_directory=source_dir,\n                      script='train.py',\n                      arguments= ... ,\n                      compute_target= ... ,\n                      environment= ... ,\n                      distributed_job_config=distr_config)\n")))}p.isMDXComponent=!0}}]);